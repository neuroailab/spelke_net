<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Discovering and using Spelke segments">
  <meta name="keywords" content="Segmentation, Computer Vision">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Discovering and using Spelke segments</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/website_logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Discovering and using Spelke segments</h1>

          <!-- Smaller font size applied here -->
          <div style="font-size: 0.75rem;">
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="#">Rahul Venkatesh</a><sup>*1</sup>,</span>
              <span class="author-block">
                <a href="#">Klemen Kotar</a><sup>*1</sup>,</span>
              <span class="author-block">
                <a href="#">Lilian Naing Chen</a><sup>*1</sup>,</span>
              <span class="author-block">
                <a href="#">Seoungwoo Kim</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="#">Luca Thomas Wheeler</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="#">Jared Watrous</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="#">Ashley Xu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="#">Gia Ancone</a><sup>1</sup>,</span>  
              <span class="author-block">
                <a href="#">Wanhee Lee</a><sup>1</sup>,</span>  
              <span class="author-block">
                <a href="#">Honglin Chen</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="#">Daniel Bear</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="#">Stefan Stojanov</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="#">Daniel Yamins</a><sup>1</sup></span>
            </div>

            <div class="is-size-6 publication-authors" style="margin-top: 0.5em;">
              <span class="author-block"><sup>1</sup>Stanford University</span>
              <span class="author-block"><sup>2</sup>OpenAI,</span>
              <span class="author-block"><sup>3</sup>Noetik Inc.</span>
            </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser_vid.mov"
                type="video/mp4">
      </video>
      <h2 class="subtitle 6 has-text-grey mt-4" style="font-style: italic;">
        <strong>Overview of SpelkeNet's Capabilities.</strong> 
        On the left, our model first predicts a probability of motion map, indicating regions likely to undergo movement. 
        We sample a point from this map and apply a virtual poke. 
        Conditioned on this intervention, our model completes the flow field, revealing object boundaries from which we extract a segment corresponding to an entity that would move together when acted on by physical forces.
        On the right, we illustrate how these discovered segments can be used in a physical object editing pipeline, 
        ensuring that edits are applied to groups of pixels that would move together in the real world, enabling more physically plausible object manipulation.
      </h2>
    </div>
  </div>
</section>

<section class="section">

  <div class="container is-max-desktop">

    <!-- SEC 1: Defining Spelke Objects -->
    <div class="content">
      <h2 class="has-text-weight-bold is-size-5 mb-4">A Better Definition of Segmentation for Physical Manipulation</h2>

      <!-- Main body paragraph -->
      <div class="content has-text-justified mt-5 is-size-6">
        <p>
          Conventional definitions of segmentation are not aligned with physical manipulation tasks.
          This can be seen in existing segmentation datasets like <a href="https://cocodataset.org/#home" target="_blank" rel="noopener noreferrer">COCO</a> and <a href="https://ade20k.csail.mit.edu/" target="_blank" rel="noopener noreferrer">ADE20K</a>, which define segments through semantic labels (e.g. car, tree, sky). 
          While useful, these categories fail to capture how objects actually move or interact physically, overlooking the critical properties that determine how segments respond to applied forces.
        </p>
        </p>
          We thus introduce the notion of <strong>Spelke objects</strong>‚Äîgroupings of physical things that reliably move together when acted on by physical forces.
          Spelke segments address this limitation by identifying groupings of pixels that respond together to forces, offering more natural alignment with physical manipulation tasks.
          Grounding the definition of segments in co-movement of pixels rather than semantic categories paves the way for a more practical, physically grounded understanding of the scene,
          which is essential for common robotics tasks like grasping, pushing, and object manipulation. 
        </p>
      </div>

    </div>

    <!-- SEC 2: Benchmarking Spelke Segments -->
    <div class="content">
      <h2 class="has-text-weight-bold is-size-5 mb-4">Benchmarking Spelke-aligned Segmentation</h2>

      <!-- Main body paragraph -->
      <div class="content has-text-justified mt-5 is-size-6">
        <p>
          To systematically evaluate segmentation algorithms against Spelke principles, we introduce <strong>SpelkeBench</strong>‚Äîa 500-image dataset that measures how well model outputs align with motion-defined object boundaries rather than semantic categories.
        </p>
        <p> 
          We curate the dataset from two complementary sources: <a href="http://luqi.info/entityv2.github.io/" target="_blank" rel="noopener noreferrer">EntitySeg</a>, featuring high-resolution internet imagery with dense segmentation annotations, and <a href="https://robotics-transformer-x.github.io/" target="_blank" rel="noopener noreferrer">Open X-Embodiment</a>, consisting of real-world egocentric robot interactions. This contrast enables evaluation across both curated image domains and physically grounded robotics environments.
        </p>
      </div>

      <!-- Image with caption -->
      <figure class="image is-inline-block has-text-centered">
        <img src="./static/images/benchmark_comparison.png"
             class="interpolation-image"
             alt="Comparison between datasets."  />
        <figcaption class="is-size-6 has-text-grey mt-2" style="font-style: italic;">
          Existing segmentation approaches like SAM and EntitySeg frequently produce segments that diverge significantly from Spelke criteria (e.g. camera subcomponents and immovable walls), 
          This is apparent when examining the segments above produced by SAM/EntitysSeg‚Äîthese segments do not represent units that we would intuit move together as a whole, demonstrating that current segmentation methods fail to capture the notion of Spelke segments.
        </figcaption>
      </figure>
    </div>

    <!-- SEC 3: Operationalizing Spelke Segments -->
    <div class="content">
      <h2 class="has-text-weight-bold is-size-5 mb-4">Operationalizing Spelke segmentation</h2>

      <!-- Subsection 1: Local Random Access Sequence Modeling (LRAS) -->
      <div class="content has-text-justified mt-5 is-size-6">
        <h3 class="has-text-weight-bold is-size-6">Local Random Access Sequence Modeling (LRAS)</h3>
        <p>
          <strong>LRAS</strong> is a sequence model inspired by large language models that causally predicts locally quantized image (RGB) and optical flow patches. The LRAS model operates on a unified vocabulary comprising RGB and flow "content" tokens and "pointer" tokens for each modality that specify spatial locations in the image grid.
          This vocabulary can be partitioned into four disjoint sets: RGB pointer tokens ùìò<sup>(rgb)</sup>, RGB content tokens ùìß, flow pointer tokens ùìò<sup>(flow)</sup>, and flow content tokens ùìï.
        </p>
        <p>
          The key innovation lies in LRAS's sequence design: each content token is paired with a corresponding pointer token specifying its spatial location, enabling sequences to be arranged in arbitrary spatial order while allowing pointer tokens to serve as modality-specific queries. This flexible, composable architecture makes LRAS particularly well-suited for discovering pixel co-movement through virtual interventions. Unlike diffusion models requiring dense global conditioning, LRAS supports sparse, localized conditioning‚Äîenabling us to inject virtual "pokes" by simply appending a flow token representing desired motion paired with a pointer token indicating spatial location. 
        </p>
        <p>
          Building on this foundation, we introduce <strong>SpelkeNet</strong>, an instance of LRAS specifically designed for Spelke object discovery through strategic sequence design that reveals which pixels move together as cohesive units under applied forces.
        </p>
      </div>
      
      <figure class="image is-inline-block has-text-centered">
        <img src="./static/images/lras_architecture.png"
            class="interpolation-image"
            alt="LRAS Architecture" />
        <figcaption class="is-size-6 has-text-grey mt-2" style="font-style: italic;">
          <strong>LRAS Architecture.</strong> SpelkeNet tokenizes images and flow fields into spatially-indexed sequences, outputs categorical distributions over flow tokens.
          Sampling from these distributions yields a complete flow field in pixel space that we use for discovering segments.
        </figcaption>
      </figure>

      <!-- Subsection 2: Motion Affordance Maps -->
      <div class="content has-text-justified mt-5 is-size-6">
        <h3 class="has-text-weight-bold is-size-6">Motion Affordance Maps</h3>
        <p>
          To discover Spelke objects, we must first identify candidate locations where virtual pokes can be applied‚Äîpixels lying on regions likely to move under external forces. We refer to this as the probability of motion affordance map, denoted <em>p</em><sub>motion</sub>. Such motion-centric affordance maps prove especially valuable in robotics applications for identifying high-motion regions that respond to interaction (e.g., cups or plates)
        </p>
        <p>
          To compute <em>p</em><sub>motion</sub>, we define a set of flow tokens corresponding to motionpanel we show that sampling from these distributions yields a complete flow field in pixel
space that we use for discovering co-moving entities/segments. greater than threshold œÑ, then sum their estimated probabilities. As flow tokens are not directly interpretable, we map each flow token to a 2D flow vector through epigraphy on the flow vocabulary‚Äîstatistically aggregating typical input flow fields that produced each token. Given RGB tokens concatenated with zero camera motion to discount camera effects, we obtain predicted flow distributions and compute motion probability at each spatial location by summing over tokens representing significant motion. This yields a 2D heatmap of regions likely to move under external forces, as illustrated below. 
        </p>
      </div>
      
      <!-- Carousel Container -->
      <div style="margin-top: 1rem; position: relative; max-width: 900px; margin-left: auto; margin-right: auto;">
        
        <!-- Carousel Images Container -->
        <div id="affordance-carousel" style="position: relative; overflow: hidden; border-radius: 8px; background: #f5f5f5;">
          <div id="carousel-images" style="display: flex; transition: transform 0.3s ease-in-out;">
            <img src="./static/images/affordancemap/1.png" style="width: 33.333%; height: 600px; object-fit: contain; flex-shrink: 0; background: white; padding: 0 5px;">
            <img src="./static/images/affordancemap/2.png" style="width: 33.333%; height: 600px; object-fit: contain; flex-shrink: 0; background: white; padding: 0 5px;">
            <img src="./static/images/affordancemap/3.png" style="width: 33.333%; height: 600px; object-fit: contain; flex-shrink: 0; background: white; padding: 0 5px;">
            <img src="./static/images/affordancemap/4.png" style="width: 33.333%; height: 600px; object-fit: contain; flex-shrink: 0; background: white; padding: 0 5px;">
            <img src="./static/images/affordancemap/5.png" style="width: 33.333%; height: 600px; object-fit: contain; flex-shrink: 0; background: white; padding: 0 5px;">
            <img src="./static/images/affordancemap/6.png" style="width: 33.333%; height: 600px; object-fit: contain; flex-shrink: 0; background: white; padding: 0 5px;">
            <img src="./static/images/affordancemap/7.png" style="width: 33.333%; height: 600px; object-fit: contain; flex-shrink: 0; background: white; padding: 0 5px;">
            <img src="./static/images/affordancemap/8.png" style="width: 33.333%; height: 600px; object-fit: contain; flex-shrink: 0; background: white; padding: 0 5px;">
          </div>
        </div>
        
        <!-- Navigation Buttons -->
        <button id="prev-btn" style="position: absolute; left: 10px; top: 50%; transform: translateY(-50%); background: rgba(0,0,0,0.7); color: white; border: none; border-radius: 50%; width: 40px; height: 40px; cursor: pointer; font-size: 18px; display: flex; align-items: center; justify-content: center; z-index: 10;">‚Äπ</button>
        <button id="next-btn" style="position: absolute; right: 10px; top: 50%; transform: translateY(-50%); background: rgba(0,0,0,0.7); color: white; border: none; border-radius: 50%; width: 40px; height: 40px; cursor: pointer; font-size: 18px; display: flex; align-items: center; justify-content: center; z-index: 10;">‚Ä∫</button>
        
        <!-- Image Counter -->
        <div id="image-counter" style="position: absolute; bottom: 10px; right: 10px; background: rgba(0,0,0,0.7); color: white; padding: 4px 8px; border-radius: 12px; font-size: 12px; z-index: 10;">1 / 8</div>
      </div>
      
      <!-- Caption -->
      <figcaption class="is-size-6 has-text-grey mt-2 has-text-centered" style="font-style: italic;">
        <strong>Motion Affordance Maps.</strong> Here we illustrate input images and their corresponding
        probability of motion heatmaps showing regions likely to exhibit motion under externally applied forces
      </figcaption>
      
      <script>
      (function() {
        let currentIndex = 0;
        const totalImages = 8;
        const imagesPerView = 3;
        const maxIndex = totalImages - imagesPerView; // 5 (positions 0-5)
        const carouselImages = document.getElementById('carousel-images');
        const imageCounter = document.getElementById('image-counter');
        const prevBtn = document.getElementById('prev-btn');
        const nextBtn = document.getElementById('next-btn');
        
        function updateCarousel() {
          const translateX = -currentIndex * (100 / imagesPerView); // Move by 33.333%
          carouselImages.style.transform = `translateX(${translateX}%)`;
          
          const startImage = currentIndex + 1;
          const endImage = Math.min(currentIndex + imagesPerView, totalImages);
          imageCounter.textContent = `${startImage}-${endImage} / ${totalImages}`;
        }
        
        function nextImage() {
          if (currentIndex < maxIndex) {
            currentIndex++;
          } else {
            currentIndex = 0; // Loop back to start
          }
          updateCarousel();
        }
        
        function prevImage() {
          if (currentIndex > 0) {
            currentIndex--;
          } else {
            currentIndex = maxIndex; // Loop to end
          }
          updateCarousel();
        }
        
        nextBtn.addEventListener('click', nextImage);
        prevBtn.addEventListener('click', prevImage);
        
        // Optional: Auto-advance every 5 seconds
        // setInterval(nextImage, 5000);
        
        // Keyboard navigation
        document.addEventListener('keydown', function(e) {
          if (e.key === 'ArrowLeft') prevImage();
          if (e.key === 'ArrowRight') nextImage();
        });
        
        // Initialize
        updateCarousel();
      })();
      </script>

      <!-- Subsection 3: Expected Displacement Maps -->
      <div class="content has-text-justified mt-5 is-size-6">
        <h3 class="has-text-weight-bold is-size-6">Expected Displacement Maps</h3>
        <p>
          Having identified regions of high motion-affordance, we leverage LRAS's sequential design to apply virtual pokes at candidate locations. We obtain the "expected displacement map"‚Äîan estimate of likely flow at each location conditioned on a virtual poke. This map provides valuable guidance about how objects might move before physical contact occurs, which can prove especially useful in robotics settings for predicting interaction outcomes.
        </p>
        <p>
          Specifically, Our approach exploits LRAS's flexible sequence composition by constructing input sequences that include virtual pokes. We represent a poke as a flow token <em>f</em><sub>k</sub> at location <em>i</em><sub>k</sub>, appending this poke to our base sequence: <strong>z</strong> = <strong>x</strong> ‚äï [<em>c</em>=0] ‚äï [(<em>i</em><sub>k</sub>, <em>f</em><sub>k</sub>)]. The model then predicts how this localized intervention propagates throughout the scene. We compute expected displacement as the probability-weighted average of flow vectors, where each vector maps to flow tokens. This yields a dense 2D vector field revealing how the entire scene would respond to the applied poke, effectively discovering which pixels move together as cohesive units.         </p>
      </div>
      
      <!-- Expected Displacement Carousel Container -->
      <div style="margin-top: 1rem; position: relative; max-width: 900px; margin-left: auto; margin-right: auto;">
        
        <!-- Carousel Images Container -->
        <div id="displacement-carousel" style="position: relative; overflow: hidden; border-radius: 8px; background: #f5f5f5;">
          <div id="displacement-carousel-images" style="display: flex; transition: transform 0.3s ease-in-out;">
            <img src="./static/images/expecteddisplacement/1.png" style="width: 33.333%; height: 600px; object-fit: contain; flex-shrink: 0; background: white; padding: 0 5px;">
            <img src="./static/images/expecteddisplacement/3.png" style="width: 33.333%; height: 600px; object-fit: contain; flex-shrink: 0; background: white; padding: 0 5px;">
            <img src="./static/images/expecteddisplacement/4.png" style="width: 33.333%; height: 600px; object-fit: contain; flex-shrink: 0; background: white; padding: 0 5px;">
            <img src="./static/images/expecteddisplacement/5.png" style="width: 33.333%; height: 600px; object-fit: contain; flex-shrink: 0; background: white; padding: 0 5px;">
            <img src="./static/images/expecteddisplacement/2_.png" style="width: 33.333%; height: 600px; object-fit: contain; flex-shrink: 0; background: white; padding: 0 5px;">
          </div>
        </div>
        
        <!-- Navigation Buttons -->
        <button id="displacement-prev-btn" style="position: absolute; left: 10px; top: 50%; transform: translateY(-50%); background: rgba(0,0,0,0.7); color: white; border: none; border-radius: 50%; width: 40px; height: 40px; cursor: pointer; font-size: 18px; display: flex; align-items: center; justify-content: center; z-index: 10;">‚Äπ</button>
        <button id="displacement-next-btn" style="position: absolute; right: 10px; top: 50%; transform: translateY(-50%); background: rgba(0,0,0,0.7); color: white; border: none; border-radius: 50%; width: 40px; height: 40px; cursor: pointer; font-size: 18px; display: flex; align-items: center; justify-content: center; z-index: 10;">‚Ä∫</button>
        
        <!-- Image Counter -->
        <div id="displacement-image-counter" style="position: absolute; bottom: 10px; right: 10px; background: rgba(0,0,0,0.7); color: white; padding: 4px 8px; border-radius: 12px; font-size: 12px; z-index: 10;">1 / 5</div>
      </div>
      
      <!-- Caption -->
      <figcaption class="is-size-6 has-text-grey mt-2 has-text-centered" style="font-style: italic;">
        <strong>Expected Displacement Maps.</strong> Vector fields showing predicted motion propagation throughout the scene from virtual poke points
      </figcaption>
      
      <script>
      (function() {
        let displacementCurrentIndex = 0;
        const displacementTotalImages = 5;
        const displacementImagesPerView = 3;
        const displacementMaxIndex = displacementTotalImages - displacementImagesPerView; // 3 (positions 0-3)
        const displacementCarouselImages = document.getElementById('displacement-carousel-images');
        const displacementImageCounter = document.getElementById('displacement-image-counter');
        const displacementPrevBtn = document.getElementById('displacement-prev-btn');
        const displacementNextBtn = document.getElementById('displacement-next-btn');
        
        function updateDisplacementCarousel() {
          const translateX = -displacementCurrentIndex * (100 / displacementImagesPerView); // Move by 33.333%
          displacementCarouselImages.style.transform = `translateX(${translateX}%)`;
          
          const startImage = displacementCurrentIndex + 1;
          const endImage = Math.min(displacementCurrentIndex + displacementImagesPerView, displacementTotalImages);
          displacementImageCounter.textContent = `${startImage}-${endImage} / ${displacementTotalImages}`;
        }
        
        function nextDisplacementImage() {
          if (displacementCurrentIndex < displacementMaxIndex) {
            displacementCurrentIndex++;
          } else {
            displacementCurrentIndex = 0; // Loop back to start
          }
          updateDisplacementCarousel();
        }
        
        function prevDisplacementImage() {
          if (displacementCurrentIndex > 0) {
            displacementCurrentIndex--;
          } else {
            displacementCurrentIndex = displacementMaxIndex; // Loop to end
          }
          updateDisplacementCarousel();
        }
        
        displacementNextBtn.addEventListener('click', nextDisplacementImage);
        displacementPrevBtn.addEventListener('click', prevDisplacementImage);
        
        // Initialize
        updateDisplacementCarousel();
      })();
      </script>
      <!-- subsection 4: Spelke Object Discovery Algorithm -->
      <div class="content">
        <h2 class="has-text-weight-bold is-size-5 mb-4">Spelke Object Discovery Algorithm</h2>
        
        <div class="content has-text-justified mt-5 is-size-6">
          <p>
            Building on the previously discussed structures, we now combine these components to discover Spelke objects through systematic "virtual pokes". Specifically, our approach leverages the sequential design of SpelkeNet to perform counterfactual probing. 
          </p>
          <p>
            We first sample a location <em>k</em> with high motion probability (<em>p</em><sub>motion</sub>(<em>k</em>) > œÑ<sub>p</sub>), then apply virtual pokes in diverse directions {<em>f</em><sup>(r)</sup>} at this location. For each poke direction, we compute the expected displacement field. To identify pixels that move in coordination with the poke, we average the dot product between each poke vector and the expected motion across all interventions. This statistical aggregation reveals regions that consistently respond together regardless of poke direction‚Äîthe hallmark of Spelke objects. Finally, Otsu thresholding extracts our desired Spelke segment, completing the discovery process from motion affordance identification through counterfactual probing to object extraction. 
          </p>
        </div>
        
        <!-- Object Discovery Carousel Container -->
        <div style="margin-top: 1rem; position: relative; max-width: 900px; margin-left: auto; margin-right: auto;">
          
          <!-- Carousel Videos Container -->
          <div id="discovery-carousel" style="position: relative; overflow: hidden; border-radius: 8px; background: #f5f5f5;">
            <div id="discovery-carousel-videos" style="display: flex; transition: transform 0.3s ease-in-out;">
              <video autoplay muted loop playsinline style="width: 100%; height: 650px; object-fit: contain; flex-shrink: 0; background: white; padding: 0 5px;">
                <source src="./static/videos/obj_discovery/3.mov" type="video/mp4">
              </video>
              <video autoplay muted loop playsinline style="width: 100%; height: 650px; object-fit: contain; flex-shrink: 0; background: white; padding: 0 5px;">
                <source src="./static/videos/obj_discovery/2.mov" type="video/mp4">
              </video>
              <video autoplay muted loop playsinline style="width: 100%; height: 650px; object-fit: contain; flex-shrink: 0; background: white; padding: 0 5px;">
                <source src="./static/videos/obj_discovery/1.mov" type="video/mp4">
              </video>
            </div>
          </div>
          
          <!-- Navigation Buttons -->
          <button id="discovery-prev-btn" style="position: absolute; left: 10px; top: 50%; transform: translateY(-50%); background: rgba(0,0,0,0.7); color: white; border: none; border-radius: 50%; width: 40px; height: 40px; cursor: pointer; font-size: 18px; display: flex; align-items: center; justify-content: center; z-index: 10;">‚Äπ</button>
          <button id="discovery-next-btn" style="position: absolute; right: 10px; top: 50%; transform: translateY(-50%); background: rgba(0,0,0,0.7); color: white; border: none; border-radius: 50%; width: 40px; height: 40px; cursor: pointer; font-size: 18px; display: flex; align-items: center; justify-content: center; z-index: 10;">‚Ä∫</button>
          
          <!-- Image Counter -->
          <div id="discovery-video-counter" style="position: absolute; bottom: 10px; right: 10px; background: rgba(0,0,0,0.7); color: white; padding: 4px 8px; border-radius: 12px; font-size: 12px; z-index: 10;">1 / 3</div>
        </div>
        
        <!-- Caption -->
        <figcaption class="is-size-6 has-text-grey mt-2 has-text-centered" style="font-style: italic;">
          <strong>Spelke Object Discovery Algorithm.</strong>
          Multiple virtual pokes are applied at locations sampled from the <em>p</em><sub>motion</sub> map. As shown, this approach can discover multiple objects within a scene and produces more meaningful segments that align with Spelke objects as compared to the SAM segments.
        </figcaption>
        
        <script>
        (function() {
          let discoveryCurrentIndex = 0;
          const discoveryTotalVideos = 3;
          const discoveryVideosPerView = 1;
          const discoveryMaxIndex = discoveryTotalVideos - discoveryVideosPerView; // 2 (positions 0-2)
          const discoveryCarouselVideos = document.getElementById('discovery-carousel-videos');
          const discoveryVideoCounter = document.getElementById('discovery-video-counter');
          const discoveryPrevBtn = document.getElementById('discovery-prev-btn');
          const discoveryNextBtn = document.getElementById('discovery-next-btn');
          
          function updateDiscoveryCarousel() {
            const translateX = -discoveryCurrentIndex * (100 / discoveryVideosPerView); // Move by 100%
            discoveryCarouselVideos.style.transform = `translateX(${translateX}%)`;
            
            const currentVideo = discoveryCurrentIndex + 1;
            discoveryVideoCounter.textContent = `${currentVideo} / ${discoveryTotalVideos}`;
          }
          
          function nextDiscoveryVideo() {
            if (discoveryCurrentIndex < discoveryMaxIndex) {
              discoveryCurrentIndex++;
            } else {
              discoveryCurrentIndex = 0; // Loop back to start
            }
            updateDiscoveryCarousel();
          }
          
          function prevDiscoveryVideo() {
            if (discoveryCurrentIndex > 0) {
              discoveryCurrentIndex--;
            } else {
              discoveryCurrentIndex = discoveryMaxIndex; // Loop to end
            }
            updateDiscoveryCarousel();
          }
          
          discoveryNextBtn.addEventListener('click', nextDiscoveryVideo);
          discoveryPrevBtn.addEventListener('click', prevDiscoveryVideo);
          
          // Initialize
          updateDiscoveryCarousel();
        })();
        </script>
        
        <!-- Quantitative Results Table -->
        <div class="content has-text-justified mt-5 is-size-6">
          <table style="margin: 0 auto; border-collapse: collapse; text-align: center;">
            <thead>
              <tr style="border-bottom: 2px solid #000;">
                <th style="padding: 8px 12px;"></th>
                <th style="padding: 8px 12px;">SAM</th>
                <th style="padding: 8px 12px;">DINOv1-B/8</th>
                <th style="padding: 8px 12px;">DINOv2-L/14</th>
                <th style="padding: 8px 12px;">DINOv2-G/14</th>
                <th style="padding: 8px 12px;">CWM</th>
                <th style="padding: 8px 12px;">SpelkeNet</th>
              </tr>
            </thead>
            <tbody>
              <tr style="border-bottom: 1px solid #ddd;">
                <td style="padding: 8px 12px; text-align: left;">AR</td>
                <td style="padding: 8px 12px;">0.4816</td>
                <td style="padding: 8px 12px;">0.2708</td>
                <td style="padding: 8px 12px;">0.2524</td>
                <td style="padding: 8px 12px;">0.2254</td>
                <td style="padding: 8px 12px;">0.3271</td>
                <td style="padding: 8px 12px; font-weight: bold;">0.5411</td>
              </tr>
              <tr style="border-bottom: 2px solid #000;">
                <td style="padding: 8px 12px; text-align: left;">mIoU</td>
                <td style="padding: 8px 12px;">0.6225</td>
                <td style="padding: 8px 12px;">0.4990</td>
                <td style="padding: 8px 12px;">0.4931</td>
                <td style="padding: 8px 12px;">0.4553</td>
                <td style="padding: 8px 12px;">0.4807</td>
                <td style="padding: 8px 12px; font-weight: bold;">0.6811</td>
              </tr>
            </tbody>
            <thead>
          </table>
          <caption class="is-size-6 has-text-grey mt-4" style="margin-bottom: 1rem; font-style: italic;">
            <strong>Quantitative evaluation of point-prompted segmentation accuracy across models:</strong> We report Average Recall (AR) and mean Intersection over Union (mIoU) for various segmentation methods. LRAS-Seg significantly outperforms both self-supervised baselines (DINO, CWM) and the supervised SAM2 model
          </caption>
        </div>
      </div>
    </div>

    <!-- SEC 5: Using Spelke Segments for Physically Plausible Object Manipulation -->
    <div class="content">
      <h2 class="has-text-weight-bold is-size-5 mb-4">Using Spelke segments for Physically Plausible Object Manipulation</h2>
      
      <div class="content has-text-justified mt-5 is-size-6">
        <p>
          Segmentation plays a crucial role in object-centric scene editing, where users click on objects and provide edit prompts for 2D or 3D transformations. 
          The segmentation model generates edit masks from these point selections, making mask quality critical for realistic results. For physically plausible scene edits, masks must reflect genuinely movable entities rather than arbitrary visual boundaries.
          As such, Spelke segments prove particularly valuable for this application, yielding more realistic scene edits as we demonstrate in our experiments with SpelkeNet.
        </p>
      </div>
      
      <!-- Object Manipulation Carousel Container -->
      <div style="margin-top: 1rem; position: relative; max-width: 900px; margin-left: auto; margin-right: auto;">
        
        <!-- Carousel Videos Container -->
        <div id="manipulation-carousel" style="position: relative; overflow: hidden; border-radius: 8px; background: #f5f5f5;">
          <div id="manipulation-carousel-images" style="display: flex; transition: transform 0.3s ease-in-out;">
            <video autoplay muted loop playsinline style="width: 100%; height: 500px; object-fit: contain; flex-shrink: 0; background: white; padding: 0 5px;">
              <source src="./static/videos/objmanipulation/1_.mov" type="video/mp4">
            </video>
            <video autoplay muted loop playsinline style="width: 100%; height: 500px; object-fit: contain; flex-shrink: 0; background: white; padding: 0 5px;">
              <source src="./static/videos/objmanipulation/2_.mov" type="video/mp4">
            </video>
            <video autoplay muted loop playsinline style="width: 100%; height: 500px; object-fit: contain; flex-shrink: 0; background: white; padding: 0 5px;">
              <source src="./static/videos/objmanipulation/3_.mov" type="video/mp4">
            </video>
          </div>
        </div>
        
        <!-- Navigation Buttons -->
        <button id="manipulation-prev-btn" style="position: absolute; left: 10px; top: 50%; transform: translateY(-50%); background: rgba(0,0,0,0.7); color: white; border: none; border-radius: 50%; width: 40px; height: 40px; cursor: pointer; font-size: 18px; display: flex; align-items: center; justify-content: center; z-index: 10;">‚Äπ</button>
        <button id="manipulation-next-btn" style="position: absolute; right: 10px; top: 50%; transform: translateY(-50%); background: rgba(0,0,0,0.7); color: white; border: none; border-radius: 50%; width: 40px; height: 40px; cursor: pointer; font-size: 18px; display: flex; align-items: center; justify-content: center; z-index: 10;">‚Ä∫</button>
        
        <!-- Image Counter -->
        <div id="manipulation-image-counter" style="position: absolute; bottom: 10px; right: 10px; background: rgba(0,0,0,0.7); color: white; padding: 4px 8px; border-radius: 12px; font-size: 12px; z-index: 10;">1 / 3</div>
      </div>
      
      <!-- Caption -->
      <figcaption class="is-size-6 has-text-grey mt-2 has-text-centered" style="font-style: italic;">
        <strong>Comparison of SpelkeNet and SAM for Object Manipulation.</strong> 
        From single-point segmentation prompt, the generated segment is used as input to an object manipulation model along with a 3D edit prompt (e.g. rotation, translation).
        We show that SpelkeNet segments enable more physically plausible object editing by respecting physical boundaries as opposed to existing segmentation methods like SAM, which often often split up or combine objects in ways that are inconsistent with how they move.
      </figcaption>
      
      <script>
      (function() {
        let manipulationCurrentIndex = 0;
        const manipulationTotalImages = 3;
        const manipulationImagesPerView = 1;
        const manipulationMaxIndex = manipulationTotalImages - manipulationImagesPerView; // 2 (positions 0-2)
        const manipulationCarouselImages = document.getElementById('manipulation-carousel-images');
        const manipulationImageCounter = document.getElementById('manipulation-image-counter');
        const manipulationPrevBtn = document.getElementById('manipulation-prev-btn');
        const manipulationNextBtn = document.getElementById('manipulation-next-btn');
        
        function updateManipulationCarousel() {
          const translateX = -manipulationCurrentIndex * (100 / manipulationImagesPerView); // Move by 100%
          manipulationCarouselImages.style.transform = `translateX(${translateX}%)`;
          
          const currentImage = manipulationCurrentIndex + 1;
          manipulationImageCounter.textContent = `${currentImage} / ${manipulationTotalImages}`;
        }
        
        function nextManipulationImage() {
          if (manipulationCurrentIndex < manipulationMaxIndex) {
            manipulationCurrentIndex++;
          } else {
            manipulationCurrentIndex = 0; // Loop back to start
          }
          updateManipulationCarousel();
        }
        
        function prevManipulationImage() {
          if (manipulationCurrentIndex > 0) {
            manipulationCurrentIndex--;
          } else {
            manipulationCurrentIndex = manipulationMaxIndex; // Loop to end
          }
          updateManipulationCarousel();
        }
        
        manipulationNextBtn.addEventListener('click', nextManipulationImage);
        manipulationPrevBtn.addEventListener('click', prevManipulationImage);
        
        // Initialize
        updateManipulationCarousel();
      })();
      </script>

      <div class="content has-text-justified mt-5 is-size-6">
        <table style="margin: 0 auto; border-collapse: collapse; text-align: center;">
          <thead>
            <tr style="border-bottom: 2px solid #000;">
              <th style="padding: 8px 12px;">Method</th>
              <th style="padding: 8px 12px;">Segment</th>
              <th style="padding: 8px 12px;">MSE ‚Üì</th>
              <th style="padding: 8px 12px;">PSNR ‚Üë</th>
              <th style="padding: 8px 12px;">LPIPS ‚Üì</th>
              <th style="padding: 8px 12px;">SSIM ‚Üë</th>
              <th style="padding: 8px 12px;">EA ‚Üë</th>
            </tr>
          </thead>
          <tbody>
            <tr style="border-bottom: 1px solid #ddd;">
              <td style="padding: 8px 12px; text-align: left;" rowspan="2">LRAS</td>
              <td style="padding: 8px 12px;">Spelke</td>
              <td style="padding: 8px 12px; font-weight: bold;">0.009</td>
              <td style="padding: 8px 12px; font-weight: bold;">21.64</td>
              <td style="padding: 8px 12px; font-weight: bold;">0.213</td>
              <td style="padding: 8px 12px; font-weight: bold;">0.698</td>
              <td style="padding: 8px 12px; font-weight: bold;">0.776</td>
            </tr>
            <tr style="border-bottom: 1px solid #ddd;">
              <td style="padding: 8px 12px;">SAM</td>
              <td style="padding: 8px 12px;">0.013</td>
              <td style="padding: 8px 12px;">20.17</td>
              <td style="padding: 8px 12px;">0.255</td>
              <td style="padding: 8px 12px;">0.685</td>
              <td style="padding: 8px 12px;">0.633</td>
            </tr>
            <tr style="border-bottom: 1px solid #ddd;">
              <td style="padding: 8px 12px; text-align: left;" rowspan="2">LightningDrag</td>
              <td style="padding: 8px 12px;">Ours</td>
              <td style="padding: 8px 12px; font-weight: bold;">0.017</td>
              <td style="padding: 8px 12px; font-weight: bold;">19.16</td>
              <td style="padding: 8px 12px; font-weight: bold;">0.195</td>
              <td style="padding: 8px 12px; font-weight: bold;">0.672</td>
              <td style="padding: 8px 12px; font-weight: bold;">0.679</td>
            </tr>
            <tr style="border-bottom: 1px solid #ddd;">
              <td style="padding: 8px 12px;">SAM</td>
              <td style="padding: 8px 12px;">0.020</td>
              <td style="padding: 8px 12px;">18.18</td>
              <td style="padding: 8px 12px;">0.241</td>
              <td style="padding: 8px 12px;">0.658</td>
              <td style="padding: 8px 12px;">0.536</td>
            </tr>
            <tr style="border-bottom: 1px solid #ddd;">
              <td style="padding: 8px 12px; text-align: left;" rowspan="2">Diffusion Handles</td>
              <td style="padding: 8px 12px;">Ours</td>
              <td style="padding: 8px 12px; font-weight: bold;">0.024</td>
              <td style="padding: 8px 12px; font-weight: bold;">17.42</td>
              <td style="padding: 8px 12px; font-weight: bold;">0.364</td>
              <td style="padding: 8px 12px; font-weight: bold;">0.555</td>
              <td style="padding: 8px 12px; font-weight: bold;">0.576</td>
            </tr>
            <tr style="border-bottom: 1px solid #ddd;">
              <td style="padding: 8px 12px;">SAM</td>
              <td style="padding: 8px 12px;">0.031</td>
              <td style="padding: 8px 12px;">16.15</td>
              <td style="padding: 8px 12px;">0.419</td>
              <td style="padding: 8px 12px;">0.526</td>
              <td style="padding: 8px 12px;">0.495</td>
            </tr>
            <tr style="border-bottom: 1px solid #ddd;">
              <td style="padding: 8px 12px; text-align: left;" rowspan="2">DiffusionAsShader</td>
              <td style="padding: 8px 12px;">Ours</td>
              <td style="padding: 8px 12px; font-weight: bold;">0.015</td>
              <td style="padding: 8px 12px; font-weight: bold;">19.29</td>
              <td style="padding: 8px 12px; font-weight: bold;">0.194</td>
              <td style="padding: 8px 12px; font-weight: bold;">0.707</td>
              <td style="padding: 8px 12px; font-weight: bold;">0.640</td>
            </tr>
            <tr style="border-bottom: 2px solid #000;">
              <td style="padding: 8px 12px;">SAM</td>
              <td style="padding: 8px 12px;">0.019</td>
              <td style="padding: 8px 12px;">18.20</td>
              <td style="padding: 8px 12px;">0.253</td>
              <td style="padding: 8px 12px;">0.682</td>
              <td style="padding: 8px 12px;">0.503</td>
            </tr>
          </tbody>
        </table>
        <caption class="is-size-6 has-text-grey mt-4" style="margin-bottom: 1rem; font-style: italic;">
          <strong>Quantitative evaluation of edit quality across segmentation methods and editing pipelines.</strong> We report results for edits generated using SAM versus SpelkeNet segments across four editing models. Lower ‚Üì is better, higher ‚Üë is better.
        </caption>
      </div>
    </div>

    <!-- SEC 6: Emergent Properties -->
    <div class="content">
      <h2 class="has-text-weight-bold is-size-5 mb-4">Emergent Properties of SpelkeNet</h2>
      
      <div class="content has-text-justified mt-5 is-size-6">
        <p>
          Beyond discovering object boundaries, SpelkeNet exhibits emergent properties that demonstrate a deeper understanding of physical scene structure.
        </p>
      </div>

      <!-- Subsection 1: Material Property Understanding -->
      <div class="content has-text-justified mt-5 is-size-6">
        
        <figure class="image is-inline-block has-text-centered">
          <img src="./static/images/material.png" alt="Material Understanding" style="height: 500px; object-fit: contain; max-width: 100%;">
          <figcaption class="is-size-6 has-text-grey mt-2" style="font-style: italic;">
            The generated probability of motion maps can be used to infer physical attributes such as rigidity or material type.
            Rigid objects like laptops and cardboard boxes tend to exhibit a
          uniform probability across the segment, while deformable objects such as cloth and plastic covers
          often show more localized motion responses near the poke point.
          </figcaption>
        </figure>
      </div>

      <!-- Subsection 2: Understanding Support Relationships -->
      <div class="content has-text-justified mt-5 is-size-6">
        <figure class="image is-inline-block has-text-centered">
          <img src="./static/images/support_relationships.png" alt="Support Relationships" style="height: 500px; object-fit: contain; max-width: 100%;">
          <figcaption class="is-size-6 has-text-grey mt-2" style="font-style: italic;">
            When applying a "virtual poke" to an object within a stack (e.g. the bottom book), the extracted Spelke segment includes both the directly contacted object and all the objects it physically supports, implying an understanding of support hierarchies within a scene.
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{venkatesh2025discoveringandusingsegments,
  title        = {Discovering and using Spelke segments}, 
  author       = {Rahul Venkatesh and Klemen Kotar and Lilian Naing Chen and Seoungwoo Kim and Luca Thomas Wheeler and Jared Watrous and Ashley Xu and Gia Ancone and Wanhee Lee and Honglin Chen and Daniel Bear and Stefan Stojanov and Daniel Yamins},
  year         = {2025},
  eprint       = {TODO},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV},
  url          = {TODO}, 
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>